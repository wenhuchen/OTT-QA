{
  "/wiki/Matrix_congruence": "In mathematics , two square matrices A and B over a field are called congruent if there exists an invertible matrix P over the same field such that",
  "/wiki/EP_matrix": "In mathematics , an EP matrix ( or range-Hermitian matrix or RPN matrix ) is a matrix A whose range is equal to the range of its conjugate transpose A* . Another equivalent characterization of EP matrices is that the range of A is orthogonal to the nullspace of A . Thus , EP matrices are also known as RPN matrices , with RPN meaning Range Perpendicular to Nullspace . EP matrices were introduced in 1950 by Hans Schwerdtfeger , and since then , many equivalent characterizations of EP matrices have been investigated through the literature . The meaning of the EP abbreviation stands originally for Equal Principal , but it is widely believed that it stands for Equal Projectors instead , since an equivalent characterization of EP matrices is based in terms of equality of the projectors AA+ and A+A . According to the fundamental theorem of linear algebra , the range of any matrix A is perpendicular to the null-space of A* , but is not necessarily perpendicular to the null-space of A . When A is an EP matrix , the range of A is precisely perpendicular to the null-space of A .",
  "/wiki/Moore\u2013Penrose_inverse": "In mathematics , and in particular linear algebra , the Moore-Penrose inverse A + { \\displaystyle A^ { + } } of a matrix A { \\displaystyle A } is the most widely known generalization of the inverse matrix . It was independently described by E. H. Moore in 1920 , Arne Bjerhammar in 1951 , and Roger Penrose in 1955 . Earlier , Erik Ivar Fredholm had introduced the concept of a pseudoinverse of integral operators in 1903 . When referring to a matrix , the term pseudoinverse , without further specification , is often used to indicate the Moore-Penrose inverse . The term generalized inverse is sometimes used as a synonym for pseudoinverse . A common use of the pseudoinverse is to compute a best fit ( least squares ) solution to a system of linear equations that lacks a unique solution ( see below under \u00a7 Applications ) . Another use is to find the minimum ( Euclidean ) norm solution to a system of linear equations with multiple solutions . The pseudoinverse facilitates the statement and proof of results in linear algebra . The pseudoinverse is defined and unique for all matrices whose entries are real or complex numbers . It can be computed using the singular value decomposition .",
  "/wiki/Idempotent_matrix": "In linear algebra , an idempotent matrix is a matrix which , when multiplied by itself , yields itself . That is , the matrix A { \\displaystyle A } is idempotent if and only if A 2 = A { \\displaystyle A^ { 2 } =A } . For this product A 2 { \\displaystyle A^ { 2 } } to be defined , A { \\displaystyle A } must necessarily be a square matrix . Viewed this way , idempotent matrices are idempotent elements of matrix rings .",
  "/wiki/Projection_(linear_algebra)": "In linear algebra and functional analysis , a projection is a linear transformation P { \\displaystyle P } from a vector space to itself such that P 2 = P { \\displaystyle P^ { 2 } =P } . That is , whenever P { \\displaystyle P } is applied twice to any value , it gives the same result as if it were applied once ( idempotent ) . It leaves its image unchanged . Though abstract , this definition of projection formalizes and generalizes the idea of graphical projection . One can also consider the effect of a projection on a geometrical object by examining the effect of the projection on points in the object .",
  "/wiki/Invertible_matrix": "In linear algebra , an n-by-n square matrix A is called invertible ( also nonsingular or nondegenerate ) if there exists an n-by-n square matrix B such that",
  "/wiki/Inverse_matrix": "In linear algebra , an n-by-n square matrix A is called invertible ( also nonsingular or nondegenerate ) if there exists an n-by-n square matrix B such that",
  "/wiki/General_linear_group": "In mathematics , the general linear group of degree n is the set of n\u00d7n invertible matrices , together with the operation of ordinary matrix multiplication . This forms a group , because the product of two invertible matrices is again invertible , and the inverse of an invertible matrix is invertible , with identity matrix as the identity element of the group . The group is so named because the columns of an invertible matrix are linearly independent , hence the vectors/points they define are in general linear position , and matrices in the general linear group take points in general linear position to points in general linear position . To be more precise , it is necessary to specify what kind of objects may appear in the entries of the matrix . For example , the general linear group over R ( the set of real numbers ) is the group of n\u00d7n invertible matrices of real numbers , and is denoted by GLn ( R ) or GL ( n , R ) . More generally , the general linear group of degree n over any field F ( such as the complex numbers ) , or a ring R ( such as the ring of integers ) , is the set of n\u00d7n invertible matrices with entries from F ( or R ) , again with matrix multiplication as the group operation . Typical notation is GLn ( F ) or GL ( n , F ) , or simply GL ( n ) if the field is understood . More generally still , the general linear group of a vector space GL ( V ) is the abstract automorphism group , not necessarily written as matrices . The special linear group , written SL ( n , F ) or SLn ( F ) , is the subgroup of GL ( n , F ) consisting of matrices with a determinant of 1 . The group GL ( n , F ) and its subgroups are often called linear groups or matrix groups ( the abstract group GL ( V ) is a linear group but not a matrix group ) . These groups are important in the theory of group representations , and also arise in the study of spatial symmetries and symmetries of vector spaces in general , as well as the study of polynomials . The modular group may be realised as a quotient of the special linear group SL ( 2 , Z ) .",
  "/wiki/Involutory_matrix": "In mathematics , an involutory matrix is a matrix that is its own inverse . That is , multiplication by matrix A is an involution if and only if A2 = I. Involutory matrices are all square roots of the identity matrix . This is simply a consequence of the fact that any nonsingular matrix multiplied by its inverse is the identity .",
  "/wiki/Signature_matrix": "In mathematics , a signature matrix is a diagonal matrix whose diagonal elements are plus or minus 1 , that is , any matrix of the form :",
  "/wiki/Householder_transformation": "In linear algebra , a Householder transformation ( also known as a Householder reflection or elementary reflector ) is a linear transformation that describes a reflection about a plane or hyperplane containing the origin . The Householder transformation was used in a 1958 paper by Alston Scott Householder . Its analogue over general inner product spaces is the Householder operator .",
  "/wiki/Nilpotent_matrix": "In linear algebra , a nilpotent matrix is a square matrix N such that",
  "/wiki/Normal_matrix": "In mathematics , a complex square matrix A is normal if it commutes with its conjugate transpose A* :",
  "/wiki/Conjugate_transpose": "In mathematics , the conjugate transpose or Hermitian transpose of an m-by-n matrix A { \\displaystyle { \\boldsymbol { A } } } with complex entries is the n-by-m matrix A H { \\displaystyle { \\boldsymbol { A } } ^ { \\mathrm { H } } } obtained from A { \\displaystyle { \\boldsymbol { A } } } by taking the transpose and then taking the complex conjugate of each entry . ( The complex conjugate of a + i b { \\displaystyle a+ib } , where a { \\displaystyle a } and b { \\displaystyle b } are real numbers , is a \u2212 i b { \\displaystyle a-ib } . )",
  "/wiki/Spectral_theorem": "In mathematics , particularly linear algebra and functional analysis , a spectral theorem is a result about when a linear operator or matrix can be diagonalized ( that is , represented as a diagonal matrix in some basis ) . This is extremely useful because computations involving a diagonalizable matrix can often be reduced to much simpler computations involving the corresponding diagonal matrix . The concept of diagonalization is relatively straightforward for operators on finite-dimensional vector spaces but requires some modification for operators on infinite-dimensional spaces . In general , the spectral theorem identifies a class of linear operators that can be modeled by multiplication operators , which are as simple as one can hope to find . In more abstract language , the spectral theorem is a statement about commutative C*-algebras . See also spectral theory for a historical perspective . Examples of operators to which the spectral theorem applies are self-adjoint operators or more generally normal operators on Hilbert spaces . The spectral theorem also provides a canonical decomposition , called the spectral decomposition , eigenvalue decomposition , or eigendecomposition , of the underlying vector space on which the operator acts . Augustin-Louis Cauchy proved the spectral theorem for self-adjoint matrices , i.e. , that every real , symmetric matrix is diagonalizable . In addition , Cauchy was the first to be systematic about determinants . The spectral theorem as generalized by John von Neumann is today perhaps the most important result of operator theory . This article mainly focuses on the simplest kind of spectral theorem , that for a self-adjoint operator on a Hilbert space .",
  "/wiki/Orthogonal_matrix": "In linear algebra , an orthogonal matrix is a square matrix whose columns and rows are orthogonal unit vectors ( orthonormal vectors ) . One way to express this is",
  "/wiki/Transpose": "In linear algebra , the transpose of a matrix is an operator which flips a matrix over its diagonal , that is it switches the row and column indices of the matrix by producing another matrix denoted as AT ( also written A\u2032 , Atr , tA or At ) . It is achieved by any one of the following equivalent actions :",
  "/wiki/Orthogonal_group": "In mathematics , the orthogonal group in dimension n ( sometimes called general orthogonal group , by analogy with the general linear group ) , denoted O ( n ) , is the group of distance-preserving transformations of a Euclidean space of dimension n that preserve a fixed point , where the group operation is given by composing transformations . Equivalently , it is the group of n\u00d7n orthogonal matrices , where the group operation is given by matrix multiplication ; an orthogonal matrix is a real matrix whose inverse equals its transpose . The orthogonal group is an algebraic group and a Lie group . It is compact . The orthogonal group in dimension n has two connected components . The one that contains the identity element is a subgroup , called the special orthogonal group , and denoted SO ( n ) . It consists of all orthogonal matrices of determinant 1 . This group is also called the rotation group , generalizing the fact that in dimensions 2 and 3 , its elements are the usual rotations around a point ( in dimension 2 ) or a line ( in dimension 3 ) . In low dimension , these groups have been widely studied , see SO ( 2 ) , SO ( 3 ) and SO ( 4 ) . In the other connected component all orthogonal matrices have -1 as a determinant . By extension , for any field F , a n\u00d7n matrix with entries in F such that its inverse equals its transpose is called an orthogonal matrix over F. The n\u00d7n orthogonal matrices form a subgroup , denoted O ( n , F ) , of the general linear group GL ( n , F ) ; that is",
  "/wiki/Orthonormal_matrix": "In linear algebra , an orthogonal matrix is a square matrix whose columns and rows are orthogonal unit vectors ( orthonormal vectors ) . One way to express this is",
  "/wiki/Orthonormal": "In linear algebra , two vectors in an inner product space are orthonormal if they are orthogonal and unit vectors . A set of vectors form an orthonormal set if all vectors in the set are mutually orthogonal and all of unit length . An orthonormal set which forms a basis is called an orthonormal basis .",
  "/wiki/Singular_matrix": "A singular matrix is a square matrix which is not invertible . Alternatively , a matrix is singular if and only if it has a determinant of 0 . When an n \u00d7 n { \\displaystyle n\\times n } matrix is taken to represent a linear transformation in n-dimensional Euclidean space , it is singular if and only if it maps any n-dimensional hypervolume to a n-dimensional hypervolume of zero volume .",
  "/wiki/Unimodular_matrix": "In mathematics , a unimodular matrix M is a square integer matrix having determinant +1 or \u22121 . Equivalently , it is an integer matrix that is invertible over the integers : there is an integer matrix N which is its inverse ( these are equivalent under Cramer 's rule ) . Thus every equation Mx = b , where M and b are both integer , and M is unimodular , has an integer solution . The unimodular matrices of order n form a group , which is denoted G L n ( Z ) { \\displaystyle GL_ { n } ( \\mathbb { Z } ) } .",
  "/wiki/Integer_matrix": "In mathematics , an integer matrix is a matrix whose entries are all integers . Examples include binary matrices , the zero matrix , the matrix of ones , the identity matrix , and the adjacency matrices used in graph theory , amongst many others . Integer matrices find frequent application in combinatorics .",
  "/wiki/Unipotent_matrix": "In mathematics , a unipotent element r of a ring R is one such that r \u2212 1 is a nilpotent element ; in other words , ( r \u2212 1 ) n is zero for some n. In particular , a square matrix , M , is a unipotent matrix , if and only if its characteristic polynomial , P ( t ) , is a power of t \u2212 1 . Thus all the eigenvalues of a unipotent matrix are 1 . The term quasi-unipotent means that some power is unipotent , for example for a diagonalizable matrix with eigenvalues that are all roots of unity . In a unipotent affine algebraic group , all elements are unipotent ( see below for the definition of an element being unipotent in such a group ) .",
  "/wiki/Unipotent_group": "In mathematics , a unipotent element r of a ring R is one such that r \u2212 1 is a nilpotent element ; in other words , ( r \u2212 1 ) n is zero for some n. In particular , a square matrix , M , is a unipotent matrix , if and only if its characteristic polynomial , P ( t ) , is a power of t \u2212 1 . Thus all the eigenvalues of a unipotent matrix are 1 . The term quasi-unipotent means that some power is unipotent , for example for a diagonalizable matrix with eigenvalues that are all roots of unity . In a unipotent affine algebraic group , all elements are unipotent ( see below for the definition of an element being unipotent in such a group ) .",
  "/wiki/Unitary_matrix": "In linear algebra , a complex square matrix U is unitary if its conjugate transpose U\u2217 is also its inverse , that is , if",
  "/wiki/Totally_unimodular_matrix": "In mathematics , a unimodular matrix M is a square integer matrix having determinant +1 or \u22121 . Equivalently , it is an integer matrix that is invertible over the integers : there is an integer matrix N which is its inverse ( these are equivalent under Cramer 's rule ) . Thus every equation Mx = b , where M and b are both integer , and M is unimodular , has an integer solution . The unimodular matrices of order n form a group , which is denoted G L n ( Z ) { \\displaystyle GL_ { n } ( \\mathbb { Z } ) } .",
  "/wiki/Linear_programming": "Linear programming ( LP , also called linear optimization ) is a method to achieve the best outcome ( such as maximum profit or lowest cost ) in a mathematical model whose requirements are represented by linear relationships . Linear programming is a special case of mathematical programming ( also known as mathematical optimization ) . More formally , linear programming is a technique for the optimization of a linear objective function , subject to linear equality and linear inequality constraints . Its feasible region is a convex polytope , which is a set defined as the intersection of finitely many half spaces , each of which is defined by a linear inequality . Its objective function is a real-valued affine ( linear ) function defined on this polyhedron . A linear programming algorithm finds a point in the polytope where this function has the smallest ( or largest ) value if such a point exists . Linear programs are problems that can be expressed in canonical form as",
  "/wiki/Linear_programming_relaxation": "In mathematics , the relaxation of a ( mixed ) integer linear program is the problem that arises by removing the integrality constraint of each variable . For example , in a 0-1 integer program , all constraints are of the form",
  "/wiki/Integer_program": "An integer programming problem is a mathematical optimization or feasibility program in which some or all of the variables are restricted to be integers . In many settings the term refers to integer linear programming ( ILP ) , in which the objective function and the constraints ( other than the integer constraints ) are linear . Integer programming is NP-complete . In particular , the special case of 0-1 integer linear programming , in which unknowns are binary , and only the restrictions must be satisfied , is one of Karp 's 21 NP-complete problems . If some decision variables are not discrete the problem is known as a mixed-integer programming problem .",
  "/wiki/Weighing_matrix": "In mathematics , a weighing matrix W of order n and weight w is an n \u00d7 n ( 0,1 , -1 ) -matrix such that W W T = w I n { \\displaystyle WW^ { T } =wI_ { n } } , where W T { \\displaystyle W^ { T } } is the transpose of W { \\displaystyle W } and I n { \\displaystyle I_ { n } } is the identity matrix of order n { \\displaystyle n } . For convenience , a weighing matrix of order n and weight w is often denoted by W ( n , w ) . A W ( n , n ) is a Hadamard matrix and a W ( n , n-1 ) is equivalent to a conference matrix ."
}